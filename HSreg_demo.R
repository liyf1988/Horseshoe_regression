######
## Compare expected risk of Ridge, PCR and HS
## Yunfan Li, December 2015-April 2016
## Yunfan Li, January-May 2017
######

library(MASS)
library(glmnet)
library(parcor)
library(sparsenet)
library(ncvreg)

sigma2 = 1
n = 100

##Sparse alpha
set.seed(2050)
index = 1:100; index = index[-c(6,30,57,67,96)]
alpha_sp = rep(0,n); alpha_sp[c(6,30,57,67,96)] = 10    #to keep inline with submitted paper
sub_index = sample(index,size=47)
alpha_sp[index] = 0.5; alpha_sp[sub_index] = -0.5

####n=number of observations
####p=number of predictors

####Generate design matrix X by a factor model
####eight factors/one factor follow Normal(0,1)
####all entries in the loading matrix B equal to 1
####each row of X is generated by the factors plus a Normal(0,a) error
xgenerate = function(k,a,n,p) {
  B <- matrix(rep(1,p*k),nrow=p,ncol=k)
  xx = matrix(0,n,p)
  for (i in (1:n)) {
    Factor <- matrix(rnorm(k,mean=0,sd=a))
    xx[i,] <- B%*%Factor+rnorm(p,mean=0,sd=0.1)
  }
  return(xx)
}

####Fix singular values
####Generate matrix tmp by random Normal, then use QR decomposition to get
####n*n and p*p orthogonal matrix, then get n*n matrix V and p*n matrix U
####fix D to be diagonal, then generate X=U*D*t(V)
eqeigan_dsgn = function(n,p){
  tmpu = matrix(rnorm(n*n,0,1),nrow=n,ncol=n)
  u = qr.Q(qr(tmpu))    #generate n*n othogonal matrix V
  tmpv = matrix(rnorm(p*n,0,1),nrow=p,ncol=n)
  v = qr.Q(qr(tmpv),complete=)
  D = diag(nrow=n,ncol=n)
  xx = u%*%D%*%t(v)
  return(xx)
}

####Function for generate training and testing data
dat_generate = function(dsgn){
  #dsgn = 'factor', 'normal', or 'eqeigen'
  xx_list = list(NA); yy_list = list(NA); D_list = list(NA); alphahat_list = list(NA); cond_num = list(NA)
  xxnew_list = list(NA); yynew_list = list(NA)
  alpha0 = alpha_sp
  for (l in 1:6) {
    if (l<6) {p = l*100}
    else if (l==6) {p=1000}
    set.seed(2045+l)  #when p=500, seed is 2050
    if (dsgn == "factor") {xx_list[[l]] = xgenerate(k=4,a=0.05,n=n,p=p)}
    #for a design matrix where the first eigenvalue is large
    else if (dsgn == "normal") {xx_list[[l]] = matrix(rnorm(n*p),nrow=n,ncol=p)}
    #for N(0,1) design matrix
    else if (dsgn == "eqeigen") {xx_list[[l]] = eqeigan_dsgn(n=n,p=p)}
    #for a design matrix with equal singular values
    svdxx = svd(x=xx_list[[l]])
    D_list[[l]] = svdxx$d
    yy_list[[l]] = svdxx$u%*%diag(D_list[[l]])%*%alpha0+rnorm(n=n,mean=0,sd=sigma)
    ####OLS estimate of alpha
    alphahat_list[[l]] = (diag(1/svdxx$d))%*%(t(svdxx$u))%*%yy_list[[l]]
    cond_num[[l]] = D_list[[l]][1]/D_list[[l]][n]
    set.seed(2046)
    for (j in 1:m) {
      if (dsgn == "factor") {xxnew_list[[(l-1)*m+j]] = xgenerate(k=4,a=0.05,n=n,p=p)}
      else if (dsgn == "normal") {xxnew_list[[(l-1)*m+j]] = matrix(rnorm(n*p),nrow=n,ncol=p)}
      else if (dsgn == "eqeigen") {xxnew_list[[(l-1)*m+j]] = eqeigan_dsgn(n=n,p=p)}
      svdxxnew = svd(x=xxnew_list[[(l-1)*m+j]])
      yynew_list[[(l-1)*m+j]] = svdxxnew$u%*%diag(svdxxnew$d)%*%alpha0+rnorm(n=n,mean=0,sd=1)
    }
  }
  return(list(xx_list=xx_list, yy_list=yy_list, alphahat_list=alphahat_list,
              D_list=D_list, xxnew_list=xxnew_list, yynew_list=yynew_list, cond_num = cond_num))
}
m = 200
sigma = sqrt(sigma2)

####Function for analyzing data
dat_analyze = function(dsgn) {
  dat = dat_generate(dsgn)
  ####RR: for each i, fitted risk=((alphahat*d)/(1+d^2*tau^2))^2, df=2*sigma^2*((d^2*tau^2)/(1+d^2*tau^2))
  RRprdsure = function(tau2){sum(((alphahat*D)/(1+D^2*tau2))^2)+2*sigma2*sum((D^2*tau2)/(1+D^2*tau2))}
  ####PCR: for i>k, fitted risk=(alphahat*d)^2; for i<=k, df=sigma^2
  PCRprdsure = function(k){sum((alphahat*D)^2)-sum(alphahat[c(1:k)]^2*D[c(1:k)]^2)+2*sigma2*k}
  ####Stein's risk of HS
  ####where X follows a CCH distribution with parameters p, q, r, s, v, theta
  a1=1          #p in CCH dist
  a2=1/2        #q in CCH dist
  x = seq(0.001,0.999,by=0.001)       #a sequence of x for integration
  ####A function of prediction risk, for optimization
  HSprdsure = function(tau2){
    sigma2 <- 1                        #true sigma^2
    H <- rep(NA,n)
    H1 <- rep(NA,n)
    H11 <- rep(NA,n)
    for (i in 1:length(alphahat)) {
      a4=(alphahat[i]^2*D[i]^2)/(2*sigma2)    #s in the CCH dist
      a6=1/(D[i]^2*tau2)                      #in the CCH dist
      #define the CCH density (without normalizing constant) for integration
      #g,g1,g11 correspond to CCH(x;p,...), CCH(x;p+1,...), CCH(x;p+2,...), respectively
      g <- function(x){x^(a1-1)*(1-x)^(a2-1)*(a6+(1-a6)*x)^(-1)*exp(-a4*x)}
      g1 <- function(x){x^(a1+1-1)*(1-x)^(a2-1)*(a6+(1-a6)*x)^(-1)*exp(-a4*x)}
      g11 <- function(x){x^(a1+2-1)*(1-x)^(a2-1)*(a6+(1-a6)*x)^(-1)*exp(-a4*x)}
      f <- function(x){x^(a1-1)*(1/a6)*exp(-a4*x)}         #f approximates g near 0
      f1 <- function(x){x^(a1+1-1)*(1/a6)*exp(-a4*x)}
      f11 <- function(x){x^(a1+2-1)*(1/a6)*exp(-a4*x)}
      if (max(g(x))>0.005){                     #if max(g)>0.005, do numerical integration
        int1 <- integrate(g,10^(-3),1-10^(-3))  #integrate g on (0.001,0.999); g on (0.999,1) approximated by exp(-a4)
        int2 <- integrate(f,0,10^(-3))          #integrate f on (0,0.001)
        H[i] <- (int1$value+int2$value+exp(-a4)*0.001)/beta(a1,a2)   #sum the integral on (0,1), divide by beta funct
        #H = H(p,q,r,s,v,theta)
        int1 <- integrate(g1,10^(-3),1-10^(-3))
        int2 <- integrate(f1,0,10^(-3))
        H1[i] <- (int1$value+int2$value+exp(-a4)*0.001)/beta(a1+1,a2)
        #H1 = H(p+1,q,r,s,v,theta)
        int1 <- integrate(g11,10^(-3),1-10^(-3))
        int2 <- integrate(f11,0,10^(-3))
        H11[i] <- (int1$value+int2$value+exp(-a4)*0.001)/beta(a1+2,a2)
        #H11 = H(p+2,q,r,s,v,theta)
      } else if (max(g(x))<=0.005){            #if max(g) small, integration err will be relatively large
        H[i] <- 1                              #set H to an arbitrary number
        H1[i] <- 0                             #set H1=0 such that E(X)=(2/3)*(H1/H)=0
        H11[i] <- 0                            #set H11=0 such that E(X^2)=(8/15)*(H11/H)=0
      }
    }
    Xmean <- (2/3)*exp(log(H1)-log(H))         #E(X), x~CCH(p,q,r,s,v,theta)
    X2mean <- (8/15)*exp(log(H11)-log(H))      #E(X^2), x~CCH(p,q,r,s,v,theta)
    #calculate prediction risk
    sum(2*sigma2-2*sigma2*Xmean+2*alphahat^2*D^2*X2mean-2*alphahat^2*D^2*Xmean^2)+sum(alphahat^2*D^2*Xmean^2)
  }  
  ####A function for prediction risk, fitted risk, 1st and 2nd moments of X~CCH
  HSest = function(tau2){
    sigma2 <- 1
    H <- rep(NA,n)
    H1 <- rep(NA,n)
    H11 <- rep(NA,n)
    for (i in 1:length(alphahat)) {
      a4=(alphahat[i]^2*D[i]^2)/(2*sigma2)    #parameter s in the CCH dist
      a6=1/(D[i]^2*tau2)                      #parameter theta in the CCH dist
      #define the CCH density (without normalizing constant) for integration
      #g,g1,g11 correspond to CCH(x;p,...), CCH(x;p+1,...), CCH(x;p+2,...), respectively
      #CCH(x;p,q,r,s,v,theta) density=g(p=a1,q=a2,r=1,s=a4,v=1,theta=a6)/(B(p,q)*H(p,q,r,s,v,theta))
      g <- function(x){x^(a1-1)*(1-x)^(a2-1)*(a6+(1-a6)*x)^(-1)*exp(-a4*x)}
      g1 <- function(x){x^(a1+1-1)*(1-x)^(a2-1)*(a6+(1-a6)*x)^(-1)*exp(-a4*x)}
      g11 <- function(x){x^(a1+2-1)*(1-x)^(a2-1)*(a6+(1-a6)*x)^(-1)*exp(-a4*x)}
      f <- function(x){x^(a1-1)*(1/a6)*exp(-a4*x)}         #f is approximate of g near 0
      f1 <- function(x){x^(a1+1-1)*(1/a6)*exp(-a4*x)}
      f11 <- function(x){x^(a1+2-1)*(1/a6)*exp(-a4*x)}
      if (max(g(x))>0.005){                     #if max(g)>0.005, do numerical integration
        int1 <- integrate(g,10^(-3),1-10^(-3))  #integrate g on (0.001,0.999)
        int2 <- integrate(f,0,10^(-3))          #integrate f on (0,0.001)
        H[i] <- (int1$value+int2$value+exp(-a4)*0.001)/beta(a1,a2)
        #calculate H(p,q,r,s,v,theta), exp(-a4) is approximate of g on (0.999,1)
        int1 <- integrate(g1,10^(-3),1-10^(-3))
        int2 <- integrate(f1,0,10^(-3))
        H1[i] <- (int1$value+int2$value+exp(-a4)*0.001)/beta(a1+1,a2)
        #calculate H(p+1,q,r,s,v,theta)
        int1 <- integrate(g11,10^(-3),1-10^(-3))
        int2 <- integrate(f11,0,10^(-3))
        H11[i] <- (int1$value+int2$value+exp(-a4)*0.001)/beta(a1+2,a2)
        #calculate H(p+2,q,r,s,v,theta)
      } else if (max(g(x))<=0.005){            #if max(g) small, integration err will be relatively large
        H[i] <- 1                              #set H to an arbitrary number
        H1[i] <- 0                             #set H1=0 such that E(X)=(2/3)*(H1/H)=0
        H11[i] <- 0                            #set H11=0 such that E(X^2)=(8/15)*(H11/H)=0
      }
    }
    Xmean <- (2/3)*exp(log(H1)-log(H))         #E(X), x~CCH(p,q,r,s,v,theta)
    X2mean <- (8/15)*exp(log(H11)-log(H))      #E(X^2), x~CCH(p,q,r,s,v,theta)
    #calculate prediction risk
    predrisk <- sum(2*sigma2-2*sigma2*Xmean+2*alphahat^2*D^2*X2mean-2*alphahat^2*D^2*Xmean^2)+sum(alphahat^2*D^2*Xmean^2)
    fitrisk <- sum(alphahat^2*D^2*Xmean^2)
    return(list(predrisk=predrisk,fitrisk=fitrisk,Xmean=Xmean,X2mean=X2mean))
  }  
  sse = matrix(ncol=7,nrow=6,NA)    #compare SSE of the methods
  ssesd = matrix(ncol=7,nrow=6,NA)    #compare sd of SSE of the methods
  cntminsse = matrix(ncol=7,nrow=6,NA)  #count of lowest SSE out of 200 testing sets
  sure = matrix(ncol=4,nrow=6,NA)   #compare SURE of RR, LASSO, PCR, HS, MCP, SCAD
  
  for (l in 1:6){
    if (l<6) {p = l*100}
    else if (l == 6) {p=1000}
    xx = dat$xx_list[[l]]
    yy = dat$yy_list[[l]]
    alphahat = dat$alphahat_list[[l]]
    D = dat$D_list[[l]]
    svdxx = svd(x=xx)
    zz = svdxx$u%*%diag(svdxx$d)

    ####Ridge regression, optimize SURE by tau^2
    set.seed(2050)
    opt = optimize(RRprdsure,c(0.01,50),maximum=F)
    tau2RR = opt$minimum
    #### Calculate estimated alpha, using optimized tau^2 and true sigma^2=1
    RRalpha = ((D^2*tau2RR)/(1+D^2*tau2RR))*alphahat
    #### Calculate fitted and predictive SURE, using optimized tau^2 and true sigma^2=1
    df = sum((D^2*tau2RR)/(1+D^2*tau2RR))
    RRsure = sum(((alphahat*D)/(1+D^2*tau2RR))^2)+2*df
    #### Estimation SSE using testing datasets
    RRssetemp = rep(NA,m)
    for (j in 1:m) {
      xxnew = dat$xxnew_list[[(l-1)*m+j]]
      yynew = dat$yynew_list[[(l-1)*m+j]]
      svdxxnew = svd(x=xxnew)
      RRyy = svdxxnew$u%*%diag(svdxxnew$d)%*%RRalpha
      RRssetemp[j] = sum((yynew-RRyy)^2)
    }
    
    ####PCR, optimize SURE by number of component K
    set.seed(2050)
    opt = optimize(PCRprdsure,c(1,n),maximum=F)
    ncomp = floor(opt$minimum)
    #### Calculate estimated alpha, using optimized K and true sigma^2=1
    PCRalpha = c(alphahat[1:ncomp],rep(0,n-ncomp))
    #### Calculate fitted and predictive SURE
    df = ncomp
    PCRsure = sum((alphahat[-c(1:ncomp)]*D[-c(1:ncomp)])^2)+2*df
    #### Calculate estimation SSE using testing dataset
    PCRssetemp = rep(NA,m)
    for (j in 1:m) {
      xxnew = dat$xxnew_list[[(l-1)*m+j]]
      yynew = dat$yynew_list[[(l-1)*m+j]]
      svdxxnew = svd(x=xxnew)
      PCRyy = svdxxnew$u%*%diag(svdxxnew$d)%*%PCRalpha
      PCRssetemp[j] = sum((yynew-PCRyy)^2)
    }
    
    ####HS, optimize Stein's risk by tau^2
    set.seed(2050)
    opt = optimize(HSprdsure,c(0.01,50),maximum=F)
    tau2HS = opt$minimum
    #### HS estimated alpha, using true sigma^2 and optimized tau^2
    Xmean = HSest(tau2HS)$Xmean          #estimate by integration
    HSalpha = (1-Xmean)*alphahat
    #### Calculate predictive SURE
    HSsure = HSest(tau2=tau2HS)$predrisk
    #### Calculate estimation SSE using testing dataset
    HSssetemp = rep(NA,m)
    for (j in 1:m) {
      xxnew = dat$xxnew_list[[(l-1)*m+j]]
      yynew = dat$yynew_list[[(l-1)*m+j]]
      svdxxnew = svd(x=xxnew)
      HSyy = svdxxnew$u%*%diag(svdxxnew$d)%*%HSalpha
      HSssetemp[j] = sum((yynew-HSyy)^2)
    }
    
    ####LASSO, estimating \alpha, optimize Stein's risk by shrinkage parameter
    set.seed(2050)
    ####Choose LASSO model by minimizing Stein's risk
    LAMBDA = seq(0.05,6,by=0.02)    #candidate shrinkage parameters
    Lstein = rep(NA,length(LAMBDA))
    for (i in 1:length(LAMBDA)){
      Lmodel = glmnet(zz,yy,alpha=1,lambda=LAMBDA[i])
      Lstein[i] = sum((yy-predict(Lmodel,newx=zz))^2)+2*sigma2*sum(Lmodel$beta!=0)  ##LASSO predictive SURE
    }
    Llambda = LAMBDA[which.min(Lstein)]    #choose the shrinkage parameter that min. SURE
    ####Best LASSO model
    Lfit = glmnet(zz,yy,alpha=1,lambda=Llambda)
    Lsure = sum((yy-predict(Lfit,newx=zz))^2)+2*sigma2*sum(Lfit$beta!=0)
    #### Estimation SSE using testing datasets
    Lssetemp = rep(NA,m)
    for (j in 1:m) {
      xxnew = dat$xxnew_list[[(l-1)*m+j]]
      yynew = dat$yynew_list[[(l-1)*m+j]]
      svdxxnew = svd(x=xxnew)
      Lyy = predict(Lfit,newx=svdxxnew$u%*%diag(svdxxnew$d))
      Lssetemp[j] = sum((yynew-Lyy)^2)
    }
    
    ####Adaptive LASSO, estimating \alpha, shrinkage parameter chosen by CV
    set.seed(2050)
    ADALmodel = adalasso(zz,yy,k=5,intercept=F)
    #### Estimation SSE using testing datasets
    ADALssetemp = rep(NA,m)
    for (j in 1:m) {
      xxnew = dat$xxnew_list[[(l-1)*m+j]]
      yynew = dat$yynew_list[[(l-1)*m+j]]
      svdxxnew = svd(x=xxnew)
      ADALyy = svdxxnew$u%*%(diag(svdxxnew$d)%*%ADALmodel$coefficients.adalasso)
      ADALssetemp[j] = sum((yynew-ADALyy)^2)
    }
    
    ####MCP, estimating \alpha, shrinkage parameter chosen by CV
    set.seed(2050)
    MCPcv = cv.sparsenet(zz,yy,nfolds=5)
    #### Estimation SSE using testing datasets
    MCPssetemp = rep(NA,m)
    for (j in 1:m) {
      xxnew = dat$xxnew_list[[(l-1)*m+j]]
      yynew = dat$yynew_list[[(l-1)*m+j]]
      svdxxnew = svd(x=xxnew)
      MCPyy = predict(MCPcv,newx=svdxxnew$u%*%diag(svdxxnew$d))    #CV selected parameters
      MCPssetemp[j] = sum((yynew-MCPyy)^2)
    }
    
    ####SCAD, estimating \alpha, shrinkage parameter chosen by CV
    set.seed(2050)
    SCADcv = cv.ncvreg(zz,yy,nfolds=5,penalty="SCAD")
    #### Estimation SSE using testing datasets
    SCADssetemp = rep(NA,m)
    for (j in 1:m) {
      xxnew = dat$xxnew_list[[(l-1)*m+j]]
      yynew = dat$yynew_list[[(l-1)*m+j]]
      svdxxnew = svd(x=xxnew)
      SCADyy = predict(SCADcv,X=svdxxnew$u%*%diag(svdxxnew$d))  #CV selected parameters
      SCADssetemp[j] = sum((yynew-SCADyy)^2)
    }
    
    sse[l,] = cbind(mean(RRssetemp),mean(PCRssetemp),mean(Lssetemp),mean(ADALssetemp),mean(MCPssetemp),mean(SCADssetemp),mean(HSssetemp))
    ssesd[l,] = cbind(sd(RRssetemp),sd(PCRssetemp),sd(Lssetemp),sd(ADALssetemp),sd(MCPssetemp),sd(SCADssetemp),sd(HSssetemp))
    sure[l,] = cbind(RRsure,PCRsure,Lsure,HSsure)
    allsse = cbind(RRssetemp,PCRssetemp,Lssetemp,ADALssetemp,MCPssetemp,SCADssetemp,HSssetemp)
    cnt = apply(allsse,1,which.min)   #which method has lowest SSE
    cntminsse[l,] = c(sum(cnt==1),sum(cnt==2),sum(cnt==3),sum(cnt==4),sum(cnt==5),sum(cnt==6),sum(cnt==7))
  }
  rownames(sse) = c('p=100','p=200','p=300','p=400','p=500','p=1000')
  rownames(ssesd) = c('p=100','p=200','p=300','p=400','p=500','p=1000')
  rownames(sure) = c('p=100','p=200','p=300','p=400','p=500','p=1000')
  rownames(cntminsse) = c('p=100','p=200','p=300','p=400','p=500','p=1000')
  colnames(sse) = c('Ridge','PCR','Lasso','AdpLasso','MCP','SCAD','HS')
  colnames(ssesd) = c('Ridge','PCR','Lasso','AdpLasso','MCP','SCAD','HS')
  colnames(sure) = c('Ridge','PCR','Lasso','HS')
  colnames(cntminsse) = c('Ridge','PCR','Lasso','AdpLasso','MCP','SCAD','HS')
  return(list(sse=sse, ssesd=ssesd, sure=sure, cntminsse=cntminsse))
}

dat_analyze(dsgn="factor")
dat_analyze(dsgn="normal")
dat_analyze(dsgn="eqeigen")

